import asyncio

import wave
from aiohttp import web
from aiortc import RTCPeerConnection, RTCSessionDescription, MediaStreamTrack

from aiortc import MediaStreamTrack


import asyncio
import os
import sherpa_onnx
import numpy as np
from pydub import AudioSegment
import webrtcvad  # éœ€è¦å…ˆå®‰è£…ï¼špip install webrtcvad
import base64  # æ·»åŠ  base64 æ¨¡å—å¯¼å…¥
import soundfile as sf

import aiohttp  # ç”¨äºå‘é€HTTPè¯·æ±‚
import json
import os
from datetime import datetime
from dotenv import load_dotenv


load_dotenv()  # åŠ è½½ç¯å¢ƒå˜é‡







# æ™ºè°±
ZHIPU_API_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
ZHIPU_API_KEY = os.getenv("ZHIPU_API_KEY", "")  # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
print(f"ZHIPU_API_KEY: {ZHIPU_API_KEY}")

# ASR æ¨¡å‹é…ç½® https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20.tar.bz2
ASR_MODEL_DIR = "./models/asr/sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20/"
ASR_ENCODER_MODEL = os.path.join(ASR_MODEL_DIR, "encoder-epoch-99-avg-1.int8.onnx")
ASR_DECODER_MODEL = os.path.join(ASR_MODEL_DIR, "decoder-epoch-99-avg-1.onnx")
ASR_JOINER_MODEL = os.path.join(ASR_MODEL_DIR, "joiner-epoch-99-avg-1.int8.onnx")
ASR_TOKENS_FILE = os.path.join(ASR_MODEL_DIR, "tokens.txt")
ASR_SAMPLE_RATE = 16000
ASR_RULE_FSTS =  os.path.join(ASR_MODEL_DIR, "itn_zh_number.fst")



#  TTS æ¨¡å‹é…ç½® https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/vits-icefall-zh-aishell3.tar.bz2
TTS_MODEL_DIR = "./models/tts/vits-icefall-zh-aishell3/"
TTS_MODEL_FILE = os.path.join(TTS_MODEL_DIR, "model.onnx")
TTS_LEXICON_FILE = os.path.join(TTS_MODEL_DIR, "lexicon.txt")
TTS_TOKENS_FILE = os.path.join(TTS_MODEL_DIR, "tokens.txt")





# åˆå§‹åŒ– WAV æ–‡ä»¶
wave_file = wave.open("output.wav", "wb")
wave_file.setnchannels(1)        # å•å£°é“
wave_file.setsampwidth(2)        # 16-bit PCM -> 2 å­—èŠ‚
wave_file.setframerate(48000)    # WebRTC é»˜è®¤é‡‡æ ·ç‡



asr_id = 0

stt_recognizer = None
tts_model = None
ACTUAL_TTS_SAMPLE_RATE = 22050 # Default, will be updated

# 1. æ¨¡å‹åˆå§‹åŒ–ä¸é…ç½®
"""
- ASR(è¯­éŸ³è¯†åˆ«)æ¨¡å‹: sherpa-onnx-streaming-zipformer-bilingual-zh-en
- TTS(è¯­éŸ³åˆæˆ)æ¨¡å‹: vits-icefall-zh-aishell3
- å¤§è¯­è¨€æ¨¡å‹: è®¯é£æ˜Ÿç«API
"""

try:
    if not all(os.path.exists(f) for f in [ASR_ENCODER_MODEL, ASR_DECODER_MODEL, ASR_JOINER_MODEL, ASR_TOKENS_FILE]):
        print(f"ASR æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {ASR_MODEL_DIR}")
    else:
        stt_recognizer = sherpa_onnx.OnlineRecognizer.from_transducer(
            tokens=ASR_TOKENS_FILE,
            encoder=ASR_ENCODER_MODEL,
            decoder=ASR_DECODER_MODEL,
            joiner=ASR_JOINER_MODEL,
            debug=True,
            # rule_fsts=ASR_RULE_FSTS,
        )
        # stt_recognizer = sherpa_onnx.OnlineRecognizer.from_transducer(
        #     tokens=ASR_TOKENS_FILE,
        #     encoder=ASR_ENCODER_MODEL,
        #     decoder=ASR_DECODER_MODEL,
        #     joiner=ASR_JOINER_MODEL,
        #     num_threads=2, debug=False, decoding_method="greedy_search", max_active_paths=4
        # )
        # stt_recognizer = sherpa_onnx.OnlineRecognizer(stt_config)
        print("Sherpa-ONNX ASR æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
except Exception as e:
    print(f"åŠ è½½ Sherpa-ONNX ASR æ¨¡å‹å¤±è´¥: {e}")


try:
    if not all(os.path.exists(f) for f in [TTS_MODEL_FILE, TTS_LEXICON_FILE, TTS_TOKENS_FILE]):
        print(f"TTS æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {TTS_MODEL_DIR}")
    else:
        tts_config = sherpa_onnx.OfflineTtsConfig(
            model=sherpa_onnx.OfflineTtsModelConfig(
                vits=sherpa_onnx.OfflineTtsVitsModelConfig(
                    model=TTS_MODEL_FILE, lexicon=TTS_LEXICON_FILE, tokens=TTS_TOKENS_FILE,
                ), num_threads=1, debug=False,
            ), rule_fsts="", max_num_sentences=1,
        )
        tts_model = sherpa_onnx.OfflineTts(tts_config)
        ACTUAL_TTS_SAMPLE_RATE = tts_model.sample_rate # è·å–æ¨¡å‹å®é™…é‡‡æ ·ç‡
        print(f"Sherpa-ONNX TTS æ¨¡å‹åŠ è½½æˆåŠŸ (é‡‡æ ·ç‡: {ACTUAL_TTS_SAMPLE_RATE} Hz)ã€‚")
except Exception as e:
    print(f"åŠ è½½ Sherpa-ONNX TTS æ¨¡å‹å¤±è´¥: {e}")



# 2. æ˜Ÿç«APIè°ƒç”¨æ¨¡å—
"""
åŠŸèƒ½: å¤„ç†ç”¨æˆ·è¯­éŸ³è¾“å…¥çš„æ–‡æœ¬ï¼Œè°ƒç”¨è®¯é£æ˜Ÿç«APIè·å–å›å¤
è¾“å…¥: è¯­éŸ³è¯†åˆ«è½¬æ¢åçš„æ–‡æœ¬
è¾“å‡º: AIåŠ©æ‰‹çš„å›å¤æ–‡æœ¬
"""
async def call_zhipu_llm_api(text):
    """è°ƒç”¨è®¯é£æ˜Ÿç«å¤§æ¨¡å‹API"""
    headers = {
        "Authorization": f"Bearer {ZHIPU_API_KEY}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": "glm-4.5-flash",
        "messages": [
            {
                "role": "user",
                "content": text
            }
        ],
        "stream": False
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(ZHIPU_API_URL, headers=headers, json=data) as response:
                if response.status == 200:
                    result = await response.json()
                    print(f"æ˜Ÿç«APIè°ƒç”¨å¤±è´¥: {result}")
                    if result.get("choices") and result["choices"][0].get("message"):
                        content = result["choices"][0]["message"]["content"]
                        return content
                    else:
                        print(f"æ˜Ÿç«APIè°ƒç”¨å¤±è´¥: {result.get('message')}")
                        return None
                else:
                    print(f"HTTPè¯·æ±‚å¤±è´¥: {response.status}")
                    return None
    except Exception as e:
        print(f"è°ƒç”¨æ˜Ÿç«APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None



# 3. éŸ³é¢‘å¤„ç†æ ¸å¿ƒç±»
"""
AudioTrackReceiverç±»
åŠŸèƒ½:
- å®æ—¶éŸ³é¢‘æµæ¥æ”¶ä¸å¤„ç†
- è¯­éŸ³æ´»åŠ¨æ£€æµ‹(VAD)
- éŸ³é¢‘æ•°æ®æ ¼å¼è½¬æ¢
- ASRå®æ—¶è¯†åˆ«
- TTSè¯­éŸ³åˆæˆ
"""
class AudioTrackReceiver(MediaStreamTrack):
    kind = "audio"

    def __init__(self, track):
        super().__init__()  # don't forget this!
        self.track = track
        self.asr_stream = stt_recognizer.create_stream() if stt_recognizer else None

        self.last_audio_time = None
        self.audio_buffer = []
        self.silence_threshold = 2.0
        self.vad = webrtcvad.Vad(3)  # çµæ•åº¦è®¾ç½®ä¸º3ï¼ˆæœ€é«˜ï¼‰
        self.frame_duration = 30  # æ¯å¸§æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰

    async def recv(self):
        global asr_id
        frame = await self.track.recv()
       
        # åœ¨è¿™é‡Œä½ å¯ä»¥å¤„ç†å¸§ï¼Œæ¯”å¦‚ä¿å­˜ PCM æ•°æ®ï¼Œåšè¯†åˆ«ç­‰
       
        # try:
        #     frame = await asyncio.wait_for(self.track.recv(), timeout=10) # PyAV AudioFrame
        # except asyncio.TimeoutError:
        #     print("ğŸ”´ éŸ³é¢‘å¸§æ¥æ”¶è¶…æ—¶")
        #     return
       
        asr_stream = self.asr_stream
        if not asr_stream:
            return frame
        
        current_time = asyncio.get_event_loop().time()
        pcm_array = frame.to_ndarray()
       # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯16-bit PCMæ ¼å¼ï¼Œå¹¶ä¸”æ˜¯ä¸€ç»´æ•°ç»„
        if pcm_array.ndim > 1:
            # å¦‚æœæ˜¯å¤šç»´æ•°ç»„ï¼Œå°†å…¶å±•å¹³ä¸ºä¸€ç»´
            pcm_array = pcm_array.flatten()
            
        if pcm_array.dtype != np.int16:
            pcm_array = (pcm_array * 32768).astype(np.int16)
        
        # è®¡ç®—æ¯å¸§æ‰€éœ€çš„æ ·æœ¬æ•°
        samples_per_frame = int(frame.sample_rate * self.frame_duration / 1000)
        
        # ç¡®ä¿éŸ³é¢‘æ•°æ®é•¿åº¦ç¬¦åˆè¦æ±‚
        if len(pcm_array) < samples_per_frame:
            # å¦‚æœå¸§å¤ªçŸ­ï¼Œç”¨é™éŸ³å¡«å……ï¼Œç¡®ä¿paddingæ˜¯ä¸€ç»´æ•°ç»„
            padding = np.zeros(samples_per_frame - len(pcm_array), dtype=np.int16)
            pcm_array = np.concatenate([pcm_array, padding])
        elif len(pcm_array) > samples_per_frame:
            # å¦‚æœå¸§å¤ªé•¿ï¼Œæˆªå–éœ€è¦çš„é•¿åº¦
            pcm_array = pcm_array[:samples_per_frame]
        
        pcm_bytes = pcm_array.tobytes()

        

        # ä½¿ç”¨VADæ£€æµ‹æ˜¯å¦æœ‰è¯­éŸ³æ´»åŠ¨
        try:
            is_speech = self.vad.is_speech(pcm_bytes, frame.sample_rate)
        except Exception as e:
            print(f"VADå¤„ç†é”™è¯¯: {e}")
            print(f"éŸ³é¢‘å‚æ•°: é‡‡æ ·ç‡={frame.sample_rate}, å¸§é•¿={len(pcm_array)}, æ•°æ®ç±»å‹={pcm_array.dtype}, ç»´åº¦={pcm_array.ndim}")
            is_speech = True  # å¦‚æœVADå¤±è´¥ï¼Œé»˜è®¤è®¤ä¸ºæ˜¯è¯­éŸ³

        if is_speech:
            self.last_audio_time = current_time
            # å°†å½“å‰å¸§çš„éŸ³é¢‘æ•°æ®æ·»åŠ åˆ°ç¼“å†²åŒº
            self.audio_buffer.append({
                'data': pcm_bytes,
                'format': frame.format,
                'sample_rate': frame.sample_rate,
                'channels': len(frame.layout.channels)
            })
            wave_file.writeframes(pcm_bytes)
  # åˆå¹¶ç¼“å†²åŒºçš„éŸ³é¢‘æ•°æ®
            combined_audio = b''.join([frame_data['data'] for frame_data in self.audio_buffer])
    

        if (current_time - (self.last_audio_time or current_time)) >= self.silence_threshold and self.audio_buffer:
            print(f"ğŸŸ¢ æ¥æ”¶åˆ°éŸ³é¢‘å¸§: {frame.samples} samples, time={frame.time}")
            self.asr_stream = stt_recognizer.create_stream() if stt_recognizer else None
            # pcm_array = frame.to_ndarray()
            # pcm_bytes = pcm_array.tobytes()
            # wave_file.writeframes(pcm_bytes)

            # full_transcript = ""
            # # è·å–éŸ³é¢‘æ•°æ®å¹¶è½¬æ¢æ ¼å¼
            # plane_data_bytes = pcm_bytes
            
            # _audio = AudioSegment(
            #     data=plane_data_bytes,
            #     sample_width=frame.format.bytes,
            #     frame_rate=frame.sample_rate,
            #     channels=len(frame.layout.channels)
            # )

             # åˆå¹¶ç¼“å†²åŒºçš„éŸ³é¢‘æ•°æ®
            combined_audio = b''.join([frame_data['data'] for frame_data in self.audio_buffer])
             # å¤„ç†åˆå¹¶åçš„éŸ³é¢‘æ•°æ®
            _audio = AudioSegment(
                data=combined_audio,
                sample_width=self.audio_buffer[0]['format'].bytes,
                frame_rate=self.audio_buffer[0]['sample_rate'],
                channels=self.audio_buffer[0]['channels']
            )
            
            # è½¬æ¢åˆ°ASRæ‰€éœ€çš„æ ¼å¼
            _audio = _audio.set_frame_rate(ASR_SAMPLE_RATE).set_channels(1)
            
            # è½¬æ¢ä¸ºfloat32æ ¼å¼å¹¶å½’ä¸€åŒ–
            samples = np.array(_audio.get_array_of_samples()).astype(np.float32) / 32768.0
            
            # é€å…¥è¯†åˆ«å™¨
            self.asr_stream.accept_waveform(ASR_SAMPLE_RATE, samples)
            
            # è§£ç å½“å‰å¯ç”¨çš„éŸ³é¢‘
            while stt_recognizer.is_ready(self.asr_stream):
                stt_recognizer.decode_stream(self.asr_stream)
            
            # è·å–å½“å‰çš„è¯†åˆ«ç»“æœ
            result = stt_recognizer.get_result(self.asr_stream)
             # æ¸…ç©ºç¼“å†²åŒºå¹¶é‡ç½®è¯†åˆ«æµ
            self.audio_buffer = []
            self.asr_stream = stt_recognizer.create_stream() if stt_recognizer else None
            if result:
                print(f"ğŸ”´ è¯†åˆ«ç»“æœ: {result}")
                for ws in ws_clients:
                    try:
                        # await ws.send_json({
                        #     'type': 'asr_result',
                        #     'text': result,
                        #     'id': asr_id
                        # })
                        asr_id = asr_id + 1
                        # ä½¿ç”¨TTSç”Ÿæˆè¯­éŸ³
                        if tts_model:
                            response = await call_zhipu_llm_api(result)
                            if response == None :
                                print("æ˜Ÿç«APIè°ƒç”¨å¤±è´¥")
                                return;
                            await ws.send_json({
                                'type': 'asr_result',
                                'text': response,
                                'id': asr_id,
                            })
                            asr_id = asr_id + 1
                            audio_data = tts_model.generate(response, sid=0, speed=1.0)
                             # å°†GeneratedAudioå¯¹è±¡è½¬æ¢ä¸ºå­—èŠ‚
                            # audio_bytes = bytes(audio_data)
                            
                            # ä¿å­˜TTSéŸ³é¢‘åˆ°WAVæ–‡ä»¶
                            tts_filename = f"tts_output.wav"
                            sf.write(
                                tts_filename,
                                audio_data.samples,
                                samplerate=audio_data.sample_rate,
                                subtype="PCM_16",
                            )
                            print(f"ğŸµ TTSéŸ³é¢‘å·²ä¿å­˜åˆ°: {tts_filename}")
                           # å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è¿›è¡Œå¤„ç†
                            audio_samples = np.array(audio_data.samples, dtype=np.float32)
                            
                            # éŸ³é‡å¢å¼º
                            audio_samples = audio_samples * 1.5  # å¢åŠ éŸ³é‡
                            # é˜²æ­¢éŸ³é¢‘è¿‡è½½
                            audio_samples = np.clip(audio_samples, -1.0, 1.0)
                            
                            # è½¬æ¢ä¸º16ä½æ•´æ•°æ ¼å¼
                            audio_samples_int16 = (audio_samples * 32767).astype(np.int16)
                            # å°†å¤„ç†åçš„éŸ³é¢‘å‘é€åˆ°å‰ç«¯
                            audio_bytes = audio_samples_int16.tobytes()
                            audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
                            # å‘é€éŸ³é¢‘æ•°æ®åˆ°å‰ç«¯
                            await ws.send_json({
                                'type': 'tts_audio',
                                'audio': audio_base64,
                                'sample_rate': audio_data.sample_rate,
                                'format': 'int16'  # å‘Šè¯‰å‰ç«¯éŸ³é¢‘æ ¼å¼
                            })
                        else:
                            print("TTSæ¨¡å‹æœªåŠ è½½")
                            # ä»…å‘é€æ–‡æœ¬ç»“æœ
                            for ws in ws_clients:
                                # await ws.send_json({
                                #     'type': 'asr_result',
                                #     'text': result
                                # })

                                audio_stream = client.text_to_speech.stream(
                                    text="This is a test",
                                    voice_id="JBFqnCBsd6RMkjVDRZzb",
                                    model_id="eleven_multilingual_v2",
                                    output_format="pcm_16000",  
                                )

                                audio_chunks = []  # å­˜å‚¨bytesæ•°æ®
                                # option 2: process the audio bytes manually
                                for chunk in audio_stream:
                                    # print(f"ğŸ”´ text_to_speech: {chunk}")
                                    if isinstance(chunk, bytes):
                                        audio_chunks.append(chunk)
                                        audio_base64 = base64.b64encode(chunk).decode('utf-8')
                                       
                                        
                                if audio_chunks:
                                    total_audio_bytes = b''.join(audio_chunks)  # åˆå¹¶bytes
                                    audio_base64 = base64.b64encode(total_audio_bytes).decode('utf-8')  # ä¸€æ¬¡æ€§ç¼–ç 
                                    
                                    await ws.send_json({
                                        'type': 'tts_audio',
                                        'audio': audio_base64,
                                        'format': 'mp3',  # âœ… ElevenLabsé»˜è®¤è¿”å›MP3
                                        'sample_rate': 16000,  # âœ… ElevenLabsé»˜è®¤é‡‡æ ·ç‡
                                        'size': len(total_audio_bytes)
                                    })
                                    print(f"ğŸµ å‘é€ElevenLabséŸ³é¢‘: {len(total_audio_bytes)} bytes")
                    except Exception as e:
                        print(f"å‘é€WebSocketæ¶ˆæ¯å¤±è´¥: {e}")
            # æ¸…ç©ºç¼“å†²åŒºå¹¶é‡ç½®è¯†åˆ«æµ
            self.audio_buffer = []
            self.asr_stream = stt_recognizer.create_stream() if stt_recognizer else None
        else:
             # å¤„ç†å®æ—¶éŸ³é¢‘æ•°æ®
            _audio = AudioSegment(
                data=pcm_bytes,
                sample_width=frame.format.bytes,
                frame_rate=frame.sample_rate,
                channels=len(frame.layout.channels)
            )
            
            # è½¬æ¢åˆ°ASRæ‰€éœ€çš„æ ¼å¼
            _audio = _audio.set_frame_rate(ASR_SAMPLE_RATE).set_channels(1)
            
            # è½¬æ¢ä¸ºfloat32æ ¼å¼å¹¶å½’ä¸€åŒ–
            samples = np.array(_audio.get_array_of_samples()).astype(np.float32) / 32768.0
            
            # é€å…¥è¯†åˆ«å™¨
            self.asr_stream.accept_waveform(ASR_SAMPLE_RATE, samples)
            
            # è§£ç å½“å‰å¯ç”¨çš„éŸ³é¢‘
            while stt_recognizer.is_ready(self.asr_stream):
                stt_recognizer.decode_stream(self.asr_stream)
            
            # è·å–å½“å‰çš„è¯†åˆ«ç»“æœ
            result = stt_recognizer.get_result(self.asr_stream)
            if result:
                print(f"ğŸ”´ è¯†åˆ«ç»“æœ: {result}")
                for ws in ws_clients:
                    try:
                        await ws.send_json({
                            'type': 'asr_result_ing',
                            'text': result,
                             'id': asr_id
                        })
                    except Exception as e:
                        print(f"å‘é€WebSocketæ¶ˆæ¯å¤±è´¥: {e}")
        return frame

routes = web.RouteTableDef()

# 5. WebRTCä¿¡ä»¤å¤„ç†
"""
åŠŸèƒ½:
- å¤„ç†WebRTCè¿æ¥å»ºç«‹
- éŸ³é¢‘æµåå•†
- ä¼šè¯æè¿°åè®®(SDP)äº¤æ¢
"""
@routes.post('/offer')
async def offer(request):
    params = await request.json()
    offer = RTCSessionDescription(sdp=params['sdp'], type=params['type'])

    pc = RTCPeerConnection()

    @pc.on('track')
    def on_track(track):
        print(f"ğŸ¤ æ”¶åˆ° track: {track.kind}")
        if track.kind == 'audio':
            audio_receiver = AudioTrackReceiver(track)

            async def process_audio():
                while True:
                    try:
                        frame = await audio_receiver.recv()
                        # å¯åœ¨æ­¤å¤„ç† frameï¼Œå¦‚å†™å…¥æ–‡ä»¶ / æ’­æ”¾ / åˆ†æ
                        
                    except Exception as e:
                        print("éŸ³é¢‘æµç»“æŸ:", e)
                        break

            asyncio.create_task(process_audio())
        @track.on('ended')
        async def on_ended():
            print('Track ended')

    await pc.setRemoteDescription(offer)
    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)

    return web.json_response({
        'sdp': pc.localDescription.sdp,
        'type': pc.localDescription.type
    })

# å­˜å‚¨æ‰€æœ‰WebSocketè¿æ¥
ws_clients = set()
# æ·»åŠ WebSocketè·¯ç”±å¤„ç†å™¨

# 4. WebSocketé€šä¿¡æ¨¡å—
"""
åŠŸèƒ½:
- ç»´æŠ¤WebSocketè¿æ¥
- å¤„ç†å®æ—¶æ¶ˆæ¯ä¼ è¾“
- å‘é€ASRè¯†åˆ«ç»“æœ
- å‘é€TTSåˆæˆéŸ³é¢‘
"""
@routes.get('/ws')
async def websocket_handler(request):
    ws = web.WebSocketResponse()
    await ws.prepare(request)
    
    # æ·»åŠ æ–°çš„WebSocketè¿æ¥åˆ°é›†åˆä¸­
    ws_clients.add(ws)
    
    try:
        async for msg in ws:
            if msg.type == web.WSMsgType.TEXT:
                if msg.data == 'close':
                    await ws.close()
            elif msg.type == web.WSMsgType.ERROR:
                print('ws connection closed with exception %s' % ws.exception())
    finally:
        ws_clients.remove(ws)
    
    return ws

@routes.get('/')
async def index(request):
    return web.FileResponse('./static/index.html')

app = web.Application()
app.add_routes(routes)

web.run_app(app, port=8000)